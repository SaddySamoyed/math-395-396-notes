\documentclass[lang=en,11pt]{template}

\usepackage{amsmath}%
\usepackage{amssymb}%
\title{Homework 5}
\author{}
\date{Due Friday September 27 at 7PM (Bonus 24 hours later)}

\begin{document}   

\chapter{hw1}

\section*{Problem A}
Suppose \((X, d)\) is a metric space. For \(0 < \epsilon < 1\), show that \(d^\epsilon\) is also a metric on \(X\).

If \(X = [0, 1]\) is the unit interval and \(d\) is the usual metric, show that \(X\) has "infinite length" using the metric \(d^\epsilon\), in that

\[
\sup_{0 = t_0 < t_1 < \cdots < t_n = 1} \sum_{i=1}^{n} d^\epsilon(t_i, t_{i-1}) = \infty.
\]

Here the supremum is taken over all \(n\) and all \(n + 1\) tuples of points \(t_i\) as in the subscript.

\textit{Optional context:} If you’d like to know where the metrics \(d^\epsilon\) appear, try looking up the Assouad Embedding Theorem. If you’d like to know more about the notion of length used here, try looking up rectifiable curves.

\section*{Bonus Problem}
If \(X\) is an \(a \times b\) matrix, and \(Y\) is a \(b \times c\) matrix, it takes \(abc\) multiplications to compute \(XY\) according to the usual formula for matrix multiplication. (There are \(ac\) entries in \(XY\), and each is a sum of \(b\) products.) Thus, let’s estimate the time it takes to multiply these two matrices as \(abc\).

Say \(A_1\) is a \(5 \times 1\) matrix, \(A_2\) is a \(1 \times 5\) matrix, \(A_3\) is a \(5 \times 2\) matrix, \(A_4\) is a \(2 \times 5\) matrix, \(A_5\) is a \(5 \times 1\) matrix, and \(A_6\) is a \(1 \times 10\) matrix.

If you want to compute \(A_1 A_2 A_3 A_4 A_5 A_6\), how should you bracket this product so that the sum of the time estimates for the multiplications is as small as possible? For example, should you do
\[
(A_1 (A_2 A_3)) ((A_4 A_5) A_6)?
\]
Or
\[
(A_1 (A_2 (A_3 (A_4 (A_5 A_6)))))?
\]
Or something else?

In general, you will have to show your work, but for this first bonus problem you only need to submit the final answer.

\textit{Hint 1:} See hint posted on my office door (EH 5848).

\textit{Hint 2:} Only use this Hint 2 if you can’t figure it out on your own after spending at least 10 minutes looking at Hint 1. In general, you can’t use Wikipedia or internet resources unless explicitly allowed, but for this first bonus problem, you can. See the Wikipedia entry titled “Matrix chain multiplication.”





\chapter{hw2}

\section*{Problem A}
If \( \|\cdot\| \) is a norm on a vector space \( V \), show that \( d(x, y) = \|x - y\| \) defines a metric on \( V \).

\section*{Problem B}
Let \( T: V_1 \to V_2 \) be a linear map between normed vector spaces. The norm on \( V_i \) will be denoted \( \|\cdot\|_i \). Define

\[
\|T\| = \sup_{v \in V_1, v \neq 0} \frac{\|T v\|_2}{\|v\|_1} = \sup_{v \in V_1, \|v\|_1 = 1} \|T v\|_2.
\]

This is either a non-negative real number or infinity. The linear map is called bounded if it is not infinity. Show that \( T \) is continuous if and only if it is bounded.

\section*{Problem C}
Give an example of an unbounded linear map.

\section*{Problem D}
Give an example of a sequence \( (T_i) \) of diagonalizable \( 2 \times 2 \) real matrices whose eigenvalues stay bounded but for which \( \|T_i\| \to \infty \). (Here the matrices define linear maps from \( \mathbb{R}^2 \) to itself, and we use the Euclidean norm on \( \mathbb{R}^2 \).)

\section*{Problem E}
Show that if a subset of a metric space is totally bounded, then it is also separable (i.e., there exists a countable dense subset).

\section*{Problem F}
Let \( X \) be defined as infinitely many copies of \( [0, 1] \) with all their left endpoints glued together, with the natural metric \( d \).

Formally, we can first define \( \hat{X} = \mathbb{N} \times [0, 1] \), and define an equivalence relation on \( \hat{X} \) by \( (i, x) \sim (j, y) \) if and only if \( (i, x) = (j, y) \) or \( x = y = 0 \). Let \( X \) be the set of equivalence classes, and define a metric \( d \) by setting

\[
d([ (i, x) ], [ (j, y) ]) = |x| + |y| \quad \text{if } i \neq j \quad \text{and} \quad d([ (i, x) ], [ (j, y) ]) = |x - y| \quad \text{if } i = j.
\]

You should convince yourself that this makes sense but don’t have to write this up.

Prove that \( (X, d) \) is bounded but not totally bounded.

\section*{Problem G}
Let \( c_0 \) be the subspace of \( \ell^\infty(\mathbb{N}) \) of sequences that converge to zero, with the sup metric. Show that a subset \( Q \) of \( c_0 \) is totally bounded if and only if it is bounded and for all \( \epsilon > 0 \), there exists \( N > 0 \) such that for all \( (x_n) \in Q \) and all \( n \geq N \), we have \( |x_n| < \epsilon \).

\section*{Bonus Problem}
A map \( f: X \to Y \) between metric spaces is called an isometric embedding if

\[
d(f(x_1), f(x_2)) = d(x_1, x_2)
\]

for all \( x_1, x_2 \in X \). If such a map exists, we say \( X \) embeds isometrically in \( Y \).

Show that every separable metric space embeds isometrically into \( \ell^\infty(\mathbb{N}) \).







\chapter{hw3}

\section*{Problem A}
Let \( (X_1, d_1) \) and \( (X_2, d_2) \) be two metric spaces. We say that a function \( f : X \to Y \) is Lipschitz with constant \( C \) if for any \( x, y \in X \), we have
\[
d_2(f(x), f(y)) \leq C d_1(x, y).
\]
\begin{enumerate}
    \item Show that Lipschitz maps are uniformly continuous, i.e., for all \( \epsilon > 0 \) there exists \( \delta > 0 \) such that if \( x_1, x_2 \in X \) and \( d_1(x_1, x_2) < \delta \), then \( d_2(f(x_1), f(x_2)) < \epsilon \).
    \item Let \( f_n : X_1 \to X_2 \) be Lipschitz maps with common Lipschitz constant \( C \). Suppose that \( f_n \) converges uniformly to \( f \), i.e., for all \( \epsilon > 0 \), there exists \( N > 0 \) such that for all \( n > N \) and all \( x \in X_1 \),
    \[
    d_2(f_n(x), f(x)) < \epsilon.
    \]
    Is \( f \) Lipschitz? What if we only assume that the \( f_n \) are Lipschitz (without giving a common Lipschitz constant)?
\end{enumerate}

\section*{Problem B}
We say that a metric space \( X \) is connected if it cannot be written as \( X = A \cup B \) where \( A \) and \( B \) are nonempty disjoint open subsets of \( X \).
\begin{enumerate}
    \item Show that if \( f : X \to Y \) is a continuous function between metric spaces \( X \) and \( Y \), then \( f(X) \) is connected if \( X \) is connected.
    \item Conclude that if \( f : X \to \mathbb{R} \) and \( X \) is a connected metric space, then \( f \) admits all intermediate values \( m \in (\inf f, \sup f) \). That is, for any such \( m \), there exists \( x_0 \in X \) such that \( f(x_0) = m \).
\end{enumerate}

\section*{Problem C}
Let \( f : X \to Y \) be a continuous bijective (one-to-one and onto) mapping between metric spaces \( X \) and \( Y \).
\begin{enumerate}
    \item Suppose that \( X \) is compact. Show that the inverse function \( f^{-1} : Y \to X \) is also continuous.
    \item Give an example to show that the requirement that \( X \) is compact is necessary.
\end{enumerate}

\section*{Problem D}
Let \( f \) be a real-valued function defined on \( \mathbb{R}^n \) (or an open subset of \( \mathbb{R}^n \)). Recall that the directional derivative \( D_v f(p) \) of \( f \) at \( p \) in the direction \( v \) is the vector
\[
D_v f(p) = \lim_{t \to 0} \frac{f(p + tv) - f(p)}{t}
\]
if this limit exists.
\begin{enumerate}
    \item If \( c \in \mathbb{R} \) and \( D_v f(p) \) exists, prove that \( D_{cv} f(p) \) exists and \( D_{cv} f(p) = c \cdot D_v f(p) \).
    \item For \( f : \mathbb{R}^2 \to \mathbb{R} \) defined by
    \[
    f(x, y) = \sqrt{|xy|}
    \]
    and \( v = (1, 0) \), \( v' = (0, 1) \), show that \( D_v f(0, 0) \) and \( D_{v'} f(0, 0) \) exist but \( D_{v+v'} f(0, 0) \) does not exist.
    \item Let \( f : \mathbb{R}^2 \to \mathbb{R} \) be defined by
    \[
    f(x, y) = \frac{xy^2}{x^2 + y^2}
    \]
    for \( (x, y) \neq (0, 0) \) and \( f(0, 0) = 0 \). Prove that \( D_v f(0, 0) \) exists for every \( v = (a, b) \in \mathbb{R}^2 \), vanishing if \( v = 0 \) and equal to
    \[
    \frac{ab^2}{a^2 + b^2}
    \]
    otherwise.
\end{enumerate}
\textit{Remark 1.} This formula for \( D_v f(0, 0) \) is not linear in \( v \).\\
\textit{Remark 2.} Using polar coordinates, it is easy to see that \( f \) is continuous at \( (0, 0) \).

\section*{Problem E}
Give the statement of the Baire Category Theorem (from Worksheet 1). (Test yourself by seeing if you can write it down from memory!)

\section*{Problem F}
Submit a write-up of Problem B from Worksheet 2.

\section*{Bonus Problem}
A metric space \( (X, d) \) is said to be uniformly disconnected if there is \( \epsilon_0 > 0 \) so that no pair of distinct points \( x, y \in X \) can be connected by an \( \epsilon_0 \)-chain, where an \( \epsilon_0 \)-chain connecting \( x \) and \( y \) is a sequence of points
\[
x = x_0, x_1, \dots, x_m = y
\]
satisfying
\[
d(x_i, x_{i+1}) \leq \epsilon_0 d(x, y).
\]
\begin{enumerate}
    \item Show that the Cantor set is uniformly disconnected.
    \item Show that a metric space \( (X, d) \) is uniformly disconnected if and only if there is an ultrametric \( d' \) on \( X \) for which there is some \( C > 1 \) such that
    \[
    \frac{d'(x, y)}{C} \leq d(x, y) \leq C d'(x, y).
    \]
\end{enumerate}
An ultrametric is a metric which satisfies the following improvement of the triangle inequality:
\[
d(x, z) \leq \max(d(x, y), d(y, z))
\]
for all \( x, y, z \). The discrete metric, where the distance between any pair of distinct points is 1, is an example of an ultrametric. Many other more interesting and important examples exist.





\chapter{hw4}

\section*{Problem A}
Let \( F : \mathbb{R}^n \to \mathbb{R}^m \) satisfy
\[
F(tx) = tF(x)
\]
for all positive real numbers \( t \) and all \( x \in \mathbb{R}^n \). Assume \( F \) is differentiable at the origin. Show that \( F \) is linear.

\section*{Problem B}
Let \( A \subset \mathbb{R}^n \) be open and \( f : A \to \mathbb{R}^m \). Suppose that the partial derivatives \( \frac{\partial f_i}{\partial x_j} \) (for \( 1 \leq i \leq m \) and \( 1 \leq j \leq n \)) exist and are bounded on \( A \). Show that \( f \) is continuous on \( A \).

\section*{Problem C}
Let \( f : \mathbb{R}^2 \to \mathbb{R}^2 \) be defined by the equation:
\[
f(r, \theta) = (r \cos \theta, r \sin \theta).
\]
\begin{enumerate}
    \item Calculate \( Df \) and \( \det Df \).
    \item Let \( S = [1, 2] \times [0, \pi/2] \). Find \( f(S) \) and sketch it.
    \item Show that \( f \) is a homeomorphism from \( S \) onto \( f(S) \) and compute the inverse function \( f^{-1} \).
    \item Compute \( Df^{-1} \) and \( \det Df^{-1} \).
    \item What relation can you find between \( Df \) and \( Df^{-1} \)?
\end{enumerate}

\section*{Problem D}
Give an example of a function \( F : \mathbb{R}^2 \to \mathbb{R}^2 \) such that, at the origin, all directional derivatives exist and are zero, but \( F \) is not differentiable at the origin.

\section*{Problem E}
Define \( f : \mathbb{R}^2 \to \mathbb{R} \) by setting \( f(0) = 0 \) and
\[
f(x, y) = \frac{xy(x^2 - y^2)}{x^2 + y^2}.
\]
\begin{enumerate}
    \item Show that \( \frac{\partial f}{\partial x} \) and \( \frac{\partial f}{\partial y} \) exist at \( (0, 0) \).
    \item Compute \( \frac{\partial f}{\partial x} \) and \( \frac{\partial f}{\partial y} \) for \( (x, y) \neq 0 \).
    \item Show that \( f \in C^1(\mathbb{R}^2) \).
    \item Show that \( \frac{\partial}{\partial x} \left( \frac{\partial f}{\partial y} \right) \) and \( \frac{\partial}{\partial y} \left( \frac{\partial f}{\partial x} \right) \) exist everywhere on \( \mathbb{R}^2 \), but they are not equal at \( (0, 0) \).
\end{enumerate}

\section*{Bonus}
Recall that an ultrametric space is a metric space where one has the following stronger than usual form of the triangle inequality:
\[
d(x, z) \leq \max(d(x, y), d(y, z)).
\]
\begin{enumerate}
    \item Show that, in an ultrametric space, open balls are closed.
    \item Show that, in an ultrametric space, if two balls intersect, one of the two must be contained in the other.
    \item Show that, in an ultrametric space, every point of a ball is the center of the ball. That is, if \( y \in B_r(x) \), then \( B_r(x) = B_r(y) \).
    \item Let \( G \) be a connected weighted undirected graph. (The weighting is the assignment of a positive number to each edge.) Let \( V(G) \) be the set of vertices. Given a path in the graph (a sequence of adjacent edges), define the length of the path to be the largest weight of an edge crossed by the path. Given \( v, w \in V(G) \), define \( d(v, w) \) to be the smallest length of a path from \( v \) to \( w \). Show that \( d \) is an ultrametric on \( V(G) \).
    \item Show that any finite ultrametric arises as in the previous part.
\end{enumerate}

\textit{Just for fun (don’t hand in):} Imagine you have an electric car, and you live in a country that provides free charging stations, and you’re not in a hurry. Why might you end up thinking about an ultrametric?



\chapter{hw5}

\section*{Problem A}
Let $F : \mathbb{R}^3 \to \mathbb{R}^3$ be defined by
\[
F(x, y, z) = (\exp(x^2 + 2y^2), \sin(z^2 - y^2) \cdot (x^2 + 2z^2), (x^2 + y^2 + z^2)^9).
\]
Explain why $F$ is differentiable, and then prove why $DF(x, y, z)$ always has zero determinant. You may not actually compute any derivatives in your solution.

\section*{Problem B}
Suppose
\[
F : A \subset \mathbb{R}^n \to B \subset \mathbb{R}^m
\]
and
\[
G : B \subset \mathbb{R}^m \to A \subset \mathbb{R}^n
\]
are both differentiable and are inverses of each other (with $A$, $B$ open). Show that $n = m$ and that, for all pairs $a \in A$, $b \in B$ with $F(a) = b$,
\[
DG(b) = DF(a)^{-1}.
\]

\section*{Problem C}
Give an example of a differentiable homeomorphism from $\mathbb{R}$ to itself whose inverse is not differentiable at every point. (For your example, you need to find only a single point where the inverse isn’t differentiable.)

\section*{Problem D}
Suppose $F : \mathbb{R}^2 \to \mathbb{R}$ is continuous at the origin. Show
\[
\lim_{h \to 0} \lim_{k \to 0} F(h, k) = \lim_{k \to 0} \lim_{h \to 0} F(h, k),
\]
assuming all these limits exist. Give an example where $F$ is not continuous, both double limits exist, but the two double limits are not equal.

\textit{Just for fun (don’t hand in)}: Give an example where $F : \mathbb{R}^2 \to \mathbb{R}$ is continuous at the origin but $\lim_{h \to 0} \lim_{k \to 0} F(h, k)$ does not exist.

\textit{Just for fun (don’t hand in)}: Also note that for $a_{n,m} = 2^{n-m}$ it is not true that
\[
\lim_{n \to \infty} \lim_{m \to \infty} a_{n,m} = \lim_{m \to \infty} \lim_{n \to \infty} a_{n,m}.
\]

\section*{Problem E}
If $F$ is a function of 4 variables, how many terms (in general) are in the degree 10 Taylor series of $F$? (Degree 10 means you use multi-indices $\alpha$ of degree at most 10.) You do not need to show your work; just give the final answer.

\section*{Problem F}
Suppose that $F : A \subset \mathbb{R}^n \to \mathbb{R}^m$ is differentiable with $A$ open and connected and $Df(a) = 0$ for all $a \in A$. Show that $F$ is constant.

\section*{Problem G}
Let $f_1, f_2, \ldots, f_m : \mathbb{R} \to \mathbb{R}$ be $C^k$. Show that
\[
\frac{\partial^k}{\partial x^k}(f_1 \cdot f_2 \cdot \ldots \cdot f_m) = \sum_{|\alpha| = k} \frac{k!}{\alpha!} \partial^{\alpha_1} f_1 \cdot \ldots \cdot \partial^{\alpha_m} f_m.
\]


$$
\sum_{i=1}^m \frac{k!}{\beta_1 ! \cdots (\beta_i- 1)! \cdots \beta_m !}
$$
is it equal to 
$$
 \frac{(k+1)!}{\beta_1 !  \cdots \beta_m !}
$$


\section*{Problem H}
Let $f : \mathbb{R}^n \to \mathbb{R}$ be in $C^{k+1}$. Show that the Taylor polynomial of degree $k$ centered at $x_0 \in \mathbb{R}^n$ is the best polynomial approximation of $f(x)$ near $x_0$ in the following sense: Suppose that $P(x)$ is a polynomial of degree $k$. Then
\[
P(x) - f(x) = o(|x - x_0|^k)
\]
if and only if $P$ is the Taylor polynomial of degree $k$ centered at $x_0$. (Recall that a quantity $Q$ is $o(|x - x_0|^k)$ if $\lim_{x \to x_0} \frac{Q}{|x - x_0|^k} = 0$.)


\section*{Problem I}
Let $f : \mathbb{R} \to \mathbb{R}$ be a differentiable function, and define $F : \mathbb{R}^2 \to \mathbb{R}$ by $F(x, y) = f(x^2 + y^2)$, so $F$ is differentiable.

\begin{enumerate}
    \item Prove that $x \frac{\partial F}{\partial y} = y \frac{\partial F}{\partial x}$.
    \item Suppose $f : \mathbb{R}^3 \to \mathbb{R}$, $g : \mathbb{R}^2 \to \mathbb{R}$, $h : \mathbb{R} \to \mathbb{R}$ are differentiable. Define $\phi : \mathbb{R}^3 \to \mathbb{R}^2$ via
    \[
    \phi(x, y, z) = (f(h(x), g(x, y), z), g(y, z)).
    \]
    Find a formula for the matrix of the derivative $D\phi(p) : \mathbb{R}^3 \to \mathbb{R}^2$ at an arbitrary point $p \in \mathbb{R}^3$ in terms of the partial derivatives of $f$, $g$, and $h$ at $p$.
    \item In (ii), compute $D\phi(1, 1, 1)$ when $f(x, y, z) = x^2 + yz$, $g(x, y) = y^3 + xy$, and $h(x) = e^x$. Do this in two ways: using your general formula in (ii) and also by explicitly computing $\phi$ in this case and directly computing the Jacobian matrix from this.
\end{enumerate}

\section*{Problem J}
Problem 2(a) on page 63 of the text.

\section*{Problem K}
Find the 3rd order Taylor series of $F(x, y) = e^{x + y^2}$ about the origin.

\section*{Bonus}
A real symmetric $n \times n$ matrix $A$ is called positive definite if $x^T A x > 0$ for all $x \in \mathbb{R}^n$.
\begin{enumerate}
    \item Show that a real symmetric matrix is positive definite if and only if it is invertible and, for all non-zero $x$, the angle between $Ax$ and $x$ is less than 90 degrees.
    \item Show that a real symmetric matrix is positive definite if and only if all its eigenvalues are positive.
    \item Let $A_d$ be the top left $d \times d$ minor of $A$. Show that if $A$ is positive definite, so is each $A_d$, $1 \leq d \leq n$.
    \item Prove that $A$ is positive definite if and only if $\det(A_d) > 0$ for all $1 \leq d \leq n$.
\end{enumerate}

You may use without proof that a real symmetric matrix has an orthonormal basis of eigenvectors. A hint for the last part is available on the office door, but as always, try it without first!

\chapter{hw6}

\noindent
\textbf{Due:} Friday, October 4 at 7 PM (Bonus 24 hours later)\\
For hints, see the office door, but try without the hints first.

\section*{Problem A}
Let $f : [a, b] \to \mathbb{R}^n$ be continuous on the closed interval $[a, b] \subset \mathbb{R}$ and differentiable on $(a, b)$.
\begin{enumerate}
    \item Show that there is a $c \in (a, b)$ such that
    \[
    |f(a) - f(b)| \leq |f'(c)| \cdot |a - b|.
    \]
    \item Give an example when $n = 2$ to show that it is possible the inequality is strict for all $c \in (a, b)$. (In particular, the Mean Value Theorem does not hold for $f$.)
\end{enumerate}

\section*{Problem B}
Let $f : \mathbb{R}^2 \to \mathbb{R}^2$ be defined by the equation
\[
f(x, y) = (e^x \cos y, e^x \sin y).
\]
\begin{enumerate}
    \item Show that $f$ is one-to-one on the set $A = \{(x, y) : x \in \mathbb{R}, 0 < y < 2\pi\}$.
    \item What is $B = f(A)$?
    \item If $g$ is the inverse function of $f$ restricted to $A$, find $Dg(0, 1)$.
    \item What is $f(\mathbb{R}^2)$?
    \item Show that the Jacobian matrix of $f$ is nonsingular for any $(x, y) \in \mathbb{R}^2$. Thus every point of $\mathbb{R}^2$ has a neighborhood on which $f$ is one-to-one. Nonetheless, show that $f$ is not one-to-one on $\mathbb{R}^2$.
    \item Find an explicit formula for the inverse function $g$ of $f$ in the neighborhood of $(0, 1)$. Use this formula to check your answer in part (3).
\end{enumerate}


Let \( f : \mathbb{R}^2 \to \mathbb{R}^2 \) be defined by
\[ f(x, y) = \left( e^x \cos y, \, e^x \sin y \right). \]

\subsection*{1. Show that \( f \) is one-to-one on the set \( A = \{ (x, y) \mid x \in \mathbb{R},\ 0 < y < 2\pi \} \).}

To prove that \( f \) is injective on \( A \), suppose that
\[
f(x_1, y_1) = f(x_2, y_2)
\]
for some \( (x_1, y_1), (x_2, y_2) \in A \). Then,
\[
\begin{cases}
e^{x_1} \cos y_1 = e^{x_2} \cos y_2, \\
e^{x_1} \sin y_1 = e^{x_2} \sin y_2.
\end{cases}
\]
Divide the second equation by the first:
\[
\frac{e^{x_1} \sin y_1}{e^{x_1} \cos y_1} = \frac{e^{x_2} \sin y_2}{e^{x_2} \cos y_2} \implies \tan y_1 = \tan y_2.
\]
Since \( 0 < y_1, y_2 < 2\pi \) and the period of \( \tan y \) is \( \pi \), we have \( y_1 = y_2 \) or \( y_1 = y_2 + \pi \) (mod \( 2\pi \)). However, adding \( \pi \) would take \( y \) out of the interval \( (0, 2\pi) \). Therefore, \( y_1 = y_2 \).

Substituting back, we have:
\[
e^{x_1} \cos y_1 = e^{x_2} \cos y_1 \implies e^{x_1} = e^{x_2}.
\]
Thus, \( x_1 = x_2 \).

\textbf{Answer:} Yes; for any two points in \( A \), if \( f(x_1, y_1) = f(x_2, y_2) \), then \( x_1 = x_2 \) and \( y_1 = y_2 \), so \( f \) is one-to-one on \( A \).

\subsection*{2. What is \( B = f(A) \)?}

As \( x \) ranges over \( \mathbb{R} \), \( e^x \) ranges over \( (0, \infty) \). As \( y \) ranges over \( (0, 2\pi) \), \( (\cos y, \sin y) \) traces the unit circle once without repeating any point (except at \( y = 0 \) and \( y = 2\pi \), which are excluded).

Therefore, \( f(A) \) covers all points in \( \mathbb{R}^2 \) except the origin \( (0, 0) \), since \( e^x > 0 \) and \( (\cos y, \sin y) \) covers all directions.

\textbf{Answer:} \( B = f(A) = \mathbb{R}^2 \setminus \{ (0, 0) \} \).

\subsection*{3. If \( g \) is the inverse function of \( f \) restricted to \( A \), find \( Dg(0, 1) \).}

First, note that \( f(0, \tfrac{\pi}{2}) = \left( e^0 \cos \tfrac{\pi}{2}, \, e^0 \sin \tfrac{\pi}{2} \right) = (0, 1) \).

Compute the Jacobian matrix \( Df \) at \( (0, \tfrac{\pi}{2}) \):
\[
Df(0, \tfrac{\pi}{2}) = \begin{bmatrix}
\frac{\partial}{\partial x} (e^x \cos y) & \frac{\partial}{\partial y} (e^x \cos y) \\
\frac{\partial}{\partial x} (e^x \sin y) & \frac{\partial}{\partial y} (e^x \sin y)
\end{bmatrix}_{(0, \frac{\pi}{2})} = \begin{bmatrix}
e^0 \cos \tfrac{\pi}{2} & -e^0 \sin \tfrac{\pi}{2} \\
e^0 \sin \tfrac{\pi}{2} & e^0 \cos \tfrac{\pi}{2}
\end{bmatrix} = \begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix}.
\]
The determinant is \( \det Df = (0)(0) - (-1)(1) = 1 \).

Since \( g \) is the inverse of \( f \), \( Dg(0, 1) \) is the inverse of \( Df(0, \tfrac{\pi}{2}) \):
\[
Dg(0, 1) = [Df(0, \tfrac{\pi}{2})]^{-1} = \begin{bmatrix}
0 & 1 \\
-1 & 0
\end{bmatrix}.
\]

\textbf{Answer:} \( Dg(0, 1) = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \).

\subsection*{4. What is \( f(\mathbb{R}^2) \)?}

For all \( x \in \mathbb{R} \) and \( y \in \mathbb{R} \), \( e^x > 0 \), and \( (\cos y, \sin y) \) traces the unit circle infinitely often due to the periodicity in \( y \).

Thus, \( f(\mathbb{R}^2) \) includes all points \( (X, Y) \) where \( X^2 + Y^2 = (e^x)^2 > 0 \), covering all of \( \mathbb{R}^2 \) except the origin.

\textbf{Answer:} \( f(\mathbb{R}^2) = \mathbb{R}^2 \setminus \{ (0, 0) \} \).

\subsection*{5. Show that the Jacobian matrix of \( f \) is nonsingular for any \( (x, y) \in \mathbb{R}^2 \). Thus, every point of \( \mathbb{R}^2 \) has a neighborhood on which \( f \) is one-to-one. Nonetheless, show that \( f \) is not one-to-one on \( \mathbb{R}^2 \).}

Compute the determinant of \( Df(x, y) \):
\[
Df(x, y) = \begin{bmatrix}
e^x \cos y & -e^x \sin y \\
e^x \sin y & e^x \cos y
\end{bmatrix}, \quad
\det Df = e^{2x} (\cos^2 y + \sin^2 y) = e^{2x} (1) = e^{2x} > 0.
\]
Since \( \det Df > 0 \) everywhere, \( Df \) is nonsingular at every point. By the Inverse Function Theorem, \( f \) is locally invertible (and thus locally one-to-one) at every point in \( \mathbb{R}^2 \).

However, \( f \) is not globally one-to-one because:
\[
f(x, y) = f(x, y + 2\pi n)
\]
for any integer \( n \), due to the periodicity of \( \cos y \) and \( \sin y \).

\textbf{Answer:} The Jacobian determinant \( \det Df(x, y) = e^{2x} > 0 \) everywhere, so \( Df \) is nonsingular for all \( (x, y) \). Therefore, \( f \) is locally one-to-one everywhere. However, \( f \) is not one-to-one on \( \mathbb{R}^2 \) because it repeats values every \( 2\pi \) in \( y \).

\subsection*{6. Find an explicit formula for the inverse function \( g \) of \( f \) in the neighborhood of \( (0, 1) \). Use this formula to check your answer in part (3).}

To find \( g \), solve \( f(x, y) = (X, Y) \) for \( x \) and \( y \).

First, compute:
\[
r = \sqrt{X^2 + Y^2} = e^x \sqrt{\cos^2 y + \sin^2 y} = e^x.
\]
Thus,
\[
x = \ln r = \ln \sqrt{X^2 + Y^2}.
\]
Next, find \( y \):
\[
\frac{Y}{X} = \tan y \implies y = \arctan\left( \frac{Y}{X} \right).
\]
To account for all quadrants, use the function \( \operatorname{atan2}(Y, X) \).

Therefore, the inverse function \( g \) is:
\[
g(X, Y) = \left( \ln \sqrt{X^2 + Y^2}, \, \operatorname{atan2}(Y, X) \right).
\]
Now, compute \( Dg \) at \( (X, Y) = (0, 1) \).

First, note that at \( (X, Y) = (0, 1) \):
\[
r = \sqrt{0^2 + 1^2} = 1, \quad x = \ln 1 = 0, \quad y = \operatorname{atan2}(1, 0) = \frac{\pi}{2}.
\]
Compute the partial derivatives:
\[
\frac{\partial x}{\partial X} = \frac{X}{X^2 + Y^2}, \quad \frac{\partial x}{\partial Y} = \frac{Y}{X^2 + Y^2}, \\
\frac{\partial y}{\partial X} = -\frac{Y}{X^2 + Y^2}, \quad \frac{\partial y}{\partial Y} = \frac{X}{X^2 + Y^2}.
\]
At \( (X, Y) = (0, 1) \), these simplify to:
\[
\frac{\partial x}{\partial X} = 0, \quad \frac{\partial x}{\partial Y} = 1, \\
\frac{\partial y}{\partial X} = -1, \quad \frac{\partial y}{\partial Y} = 0.
\]
Thus,
\[
Dg(0, 1) = \begin{bmatrix}
0 & 1 \\
-1 & 0
\end{bmatrix}.
\]
This matches the result from part (3).

\textbf{Answer:} An explicit formula is
\[
g(X, Y) = \left( \ln \sqrt{X^2 + Y^2}, \, \operatorname{atan2}(Y, X) \right).
\]
Using this formula, we find \( Dg(0, 1) = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \), confirming our answer in part (3).


\section*{Problem C}
Suppose that $f : \mathbb{R} \to \mathbb{R}$ is continuous and locally invertible. Show that the image of $f$ is open, and that a global inverse for $f$ exists defined on the image.

\textit{Remark:} The analogue of Problem C is false in higher dimensions. You should pause to note what your solution uses that wouldn’t be available in higher dimensions.

\textit{Just for fun (don’t hand in):} Give an example of a continuous $f : \mathbb{R}^2 \to \mathbb{R}^2$ that is locally invertible but not injective.

\section*{Problem D}
Say that $f : \mathbb{R}^m \to \mathbb{R}$ is $C^\infty$, and there is a number $W > 0$ such that for all $x \in B_r(0)$ and all $\alpha$, we have
\[
|\partial^\alpha f(x)| \leq W |\alpha|.
\]
Show that
\[
\lim_{k \to \infty} \sum_{|\alpha| \leq k} \frac{\partial^\alpha f(0)}{\alpha!} x^\alpha = f(x)
\]
for $x \in B_r(0)$. Show also that the infinite series
\[
\sum_{\alpha} \frac{\partial^\alpha f(0)}{\alpha!} x^\alpha
\]
converges absolutely, so that without any ambiguity we can write
\[
f(x) = \sum_{\alpha} \frac{\partial^\alpha f(0)}{\alpha!} x^\alpha.
\]

\textit{Just for fun (don’t hand in):} $|\partial^\alpha f(x)| \leq W |\alpha|$ is not optimal. Can you phrase a natural assumption that is closer to optimal?

\section*{Problem E}
If $U \subset \mathbb{R}^n$ is open and $f : U \to \mathbb{R}$ is $C^1$ and has a local minimum at $x$, prove $Df(x) = 0$. (Points $x$ with $Df(x) = 0$ are called critical points.)

\section*{Problem F}
Let $f : A \to \mathbb{R}$ be a $C^2$ function, $A \subset \mathbb{R}^n$ open, and let $x \in A$. The Hessian $Hf(x)$ of $f$ at $x$ is the $n$-by-$n$ symmetric matrix with entry $(j, k)$ equal to $\partial e_i + e_k f(x)$.

A symmetric matrix $S$ is called positive definite if $x^T S x > 0$ for all $x \in \mathbb{R}^n$, $x \neq 0$, in which case we write $S > 0$. If the same condition holds with a non-strict inequality $x^T S x \geq 0$, we say $S$ is positive semi-definite and write $S \geq 0$. The negative of a positive (semi-)definite matrix is called negative (semi-)definite, and we similarly write $S < 0$ or $S \leq 0$.

Assume that $A$ is convex, and $Df(x_0) = 0$. Prove that:
\begin{enumerate}
    \item if $Hf(x) \geq 0$ for all $x \in A$, then $f(x) \geq f(x_0)$ for all $x \in A$.
    \item if $Hf(x) > 0$ for all $x \in A$, then $f(x) > f(x_0)$ for all $x \in A$.
    \item if $Hf(x) \leq 0$ for all $x \in A$, then $f(x) \leq f(x_0)$ for all $x \in A$.
    \item if $Hf(x) < 0$ for all $x \in A$, then $f(x) < f(x_0)$ for all $x \in A$.
    \item if $Hf(x_0) \not\geq 0$, then $f$ does not have a local minimum at $x_0$.
    \item if $Hf(x_0) \not\leq 0$, then $f$ does not have a local maximum at $x_0$.
\end{enumerate}

You only need to submit your proofs for parts (1) and (5); you don’t have to write up the rest.

\section*{Problem G}
Submit a write-up for Problem E on worksheet 5.

\section*{Bonus}
Let $M_n(\mathbb{R})$ denote the set of $n$-by-$n$ real matrices. It has a topology and metric by identifying with $\mathbb{R}^{n^2}$ using the entries of the metric.
\begin{enumerate}
    \item For any $A \in M_n(\mathbb{R})$, show that
    \[
    \lim_{K \to \infty} \sum_{k=0}^K \frac{A^k}{k!}
    \]
    exists. This limit is denoted
    \[
    \exp(X) = \sum_{k=0}^\infty \frac{A^k}{k!}.
    \]
    \item Compute $\exp$ of the following matrices:
    \[
    \begin{pmatrix} 0 & t \\ 0 & 0 \end{pmatrix}, \quad \begin{pmatrix} s & 0 \\ 0 & t \end{pmatrix}, \quad \begin{pmatrix} 0 & t \\ -t & 0 \end{pmatrix}.
    \]
    \item Show that $\exp(A + B) = \exp(A)\exp(B)$ when $A$ and $B$ commute.
    \item Prove that $\exp$ is differentiable at the origin and compute its derivative there.
    \item Is $\exp$ surjective?
    \item Is $\exp$ injective?
\end{enumerate}

\chapter{hw7}

For hints, see office door. But try without the hints first. There are no IBL problems this week.

\section*{Problem A}
Suppose $A \subset \mathbb{R}^n$ is open and $f : A \to \mathbb{R}$ is differentiable at $x \in A$. Show that if $u$ is a unit vector in $\mathbb{R}^n$, then
\[
D_u f(x) \leq |Df(x)|,
\]
with equality if and only if $u = \frac{Df(x)}{|Df(x)|}$. Show that $D_u f(x) = 0$ if and only if $u$ is orthogonal to $Df(x)$.

\textbf{Remark}: Keeping in mind that for functions to $\mathbb{R}$, the Jacobian matrix is also called the gradient, this shows that the gradient is the direction of fastest change of the function. Similarly, the set of directions where the function does not change (to first order) is the perpendicular space to the gradient.

\section*{Problem B}
Suppose $U \subset \mathbb{R}^n$ is open and $f : U \to \mathbb{R}$ and $c : U \to \mathbb{R}$ are $C^1$. Set
\[
M = c^{-1}(0).
\]
Assume that $f$ restricted to $M$ has a local minimum at $p$, and that $Dc$ is surjective at $p$. Then prove that there exists a real number $\lambda$ such that
\[
Df(p) = \lambda Dc(p).
\]
(This means the gradients of $f$ and $c$ are parallel at $p$. The number $\lambda$ is called a \textit{Lagrange multiplier}.)

Hints: how that M can be parameterized, by implicit function theorem: M = {(x, g(x))} for a $g: \mathbb{R}^{n-1} \rightarrow \mathbb{R}$, show that kerDc = ImDg


\section*{Problem C}
In at most a few sentences, give a non-rigorous, intuitive explanation for Problem B.

\section*{Problem D}
Using Problem B, find the minimum of the function $f(x, y) = 3x + y$ on the unit circle centered at the origin in $\mathbb{R}^2$.

\section*{Problem E}
Formulate and prove a generalization of Problem B when $c$ maps to $\mathbb{R}^k$ rather than $\mathbb{R}$. (Whereas Problem B allows you to do certain optimization problems subject to one constraint, this lets you do some optimization problems subject to $k$ constraints. Your generalization will feature numbers $\lambda_1, \dots, \lambda_k$.)

\textbf{Remark}: You must do Problem B first and then Problem E. You may not reference E in your solution to B.

\section*{Problem F}
Prove that the set of positive definite matrices is open in the set of $n \times n$ symmetric matrices. (You may not use the bonus from HW5.)

\section*{Problem G}
Suppose $f : A \to \mathbb{R}$ is $C^2$, with $A \subset \mathbb{R}^n$ open. Suppose that $x_0$ is a critical point of $f$ and the Hessian of $f$ is positive definite at $x_0$. Prove that $x_0$ is a strict local minimum for $f$.

\section*{Problem H}
Let $A$ be an invertible $n \times n$ matrix. Let $C$ be its cofactor matrix, so $C_{ij} = (-1)^{i+j} \det(A_{ij})$, where $A_{ij}$ is the $(n-1) \times (n-1)$ matrix obtained by deleting row $i$ and column $j$ from $A$. Prove the following version of Cramer's rule:
\[
A^{-1} = \frac{1}{\det(A)} C^T,
\]
where $C^T$ denotes the transpose of $C$. (You may use the cofactor expansion of the determinant.)

\section*{Problem I}
Let $f, g : (a, b) \subset \mathbb{R} \to \mathbb{R}^n$ be differentiable. Show that
\[
(f \cdot g)'(t) = f'(t) \cdot g(t) + f(t) \cdot g'(t),
\]
where $\cdot$ denotes dot product and $f'(t)$ denotes $Df(t)$ (which in this case is a vector).

\section*{Bonus}
Suppose $f : A \to \mathbb{R}$ is $C^2$, with $A \subset \mathbb{R}^n$ open and convex. Show that the region above $f$, i.e.
\[
\{(x, y) \in A \times \mathbb{R} : y \geq f(x)\},
\]
is convex if and only if the Hessian of $f$ is positive semi-definite at each point of $A$.




\chapter{hw8}

\section*{Problem A}
Let \( f : \mathbb{R}^3 \to \mathbb{R}^2 \) be of class \( C^1 \) and such that \( f(1, 2, 3) = 0 \) and

\[
Df(1, 2, 3) =
\begin{bmatrix}
1 & 2 & 1 \\
1 & -1 & 1
\end{bmatrix}
.
\]

\begin{enumerate}
    \item Does the equation \( f(x, y, z) = 0 \) define implicitly a function of some of the variables in terms of the rest? If so, what variables can be expressed in terms of what others? Discuss all the possibilities.
    \item Suppose there is a function \( g : B \to \mathbb{R}^2 \) of class \( C^1 \) defined on an open set \( B \) of \( \mathbb{R} \) such that \( f(x, g(x)) = 0 \) for \( x \in B \) and \( g(1) = (2, 3) \). Compute \( Dg(1) \).
\end{enumerate}

\section*{Problem B}
Let \( f : \mathbb{R}^{k+n} \to \mathbb{R}^n \) be of class \( C^1 \) and suppose that \( f(a) = 0 \) and \( Df(a) \) has rank \( n \). Show that if \( c \in \mathbb{R}^n \) is sufficiently close to 0, then the equation \( f(x) = c \) has a solution.

\section*{Problem C}
Let \( B \) be a closed box, and \( f : B \to \mathbb{R} \) be a continuous function. Show that \( f \) is integrable.

\section*{Problem D}
Write up Problem D from the IBL worksheet (on countable sets).

\section*{Problem E}
Write up Problem G from the IBL worksheet (on countable sets).

\section*{Bonus}
A map \( T \) from a metric space \( (X, d) \) to itself is called a contraction mapping if there is a \( 0 \leq c < 1 \) such that
\[
d(T(x), T(y)) \leq c \cdot d(x, y)
\]
for all \( x, y \in X \).

\begin{enumerate}
    \item Show that every contraction mapping of a complete metric space has a unique fixed point.
    \item Suppose that \( f : \mathbb{R}^d \to \mathbb{R}^d \) is \( C^1 \), and \( Df(0) \) is invertible. Show that for \( \epsilon > 0 \) sufficiently small, if \( B \) is the closed ball of radius \( \epsilon \) around 0, then there is a \( \delta > 0 \) such that if \( |y| < \delta \)
    \[
    T(x) = Df(0)(x) + y - f(x)
    \]
    defines a contraction mapping from \( B \) to itself. (As part of this, you’ll have to show \( T(B) \subset B \).)
    \item Explain why this immediately implies that the image of \( f \) contains a neighborhood of \( f(0) \).
\end{enumerate}

\textit{Remark:} The point of this problem is to give the idea for a different proof of the Inverse Function Theorem. (This proof can be found in many textbooks, but don’t look!) Studying the proof from class will help you solve this question, and you can use the lemmas from class if you want to.













\end{document}
